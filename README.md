
# USP-PPGSI-SIN5021

Repository for the SIN5021 graduate course Planning and Reinforcement Learning

# Reports:

 1. [Avaliação de algoritmos para MDPs](Report1/README.md)
 2. TBD
 
# Ementa da disciplina

**Objetivo**: Estudar os fundamentos teóricos básicos, técnicas e principais abordagens usadas em planejamento probabilístico e aprendizado por reforço em Inteligência Artificial.<br/>
**Justificativa da disciplina**: Esta disciplina optativa visa complementar a formação dos estudantes, especificamente aqueles interessados na área de Inteligência Artificial. Planejamento é um dos desafios mais importantes dessa área que permite incrementar a autonomia e flexibilidade de sistemas inteligentes e envolve gerar automaticamente uma sequência de ações para transformar uma situação inicial em uma situação final. As técnicas de planejamento têm sido aplicadas em uma variedade ampla de tarefas, entre elas: robótica, planejamento de processos industriais, logística/transporte, composição de serviços web, planejamento para recuperação de informação, planejamento de agentes autônomos, entre outras. O aprendizado por reforço é uma solução alternativa às tarefas citadas quando a dinâmica das tarefas não pode ser obtida a priori. Dessa forma, o aprendizado por reforço resolve o problema de planejamento por meio de interações com a tarefa em questão utilizando uma estratégia de tentativa e erro.<br/>
**Carga horária**: 120<br/>
**Número de créditos**: 8<br/>
**Programa**: Serão abordados os seguintes tópicos: o problema de planejamento, planejamento clássico e planejamento probabilístico, processos de decisão markovianos (MDP) e a existência de soluções, algoritmos de Iteração de Política e Iteração de Valor, processos de decisão markovianos fatorados e algoritmos básicos, algoritmos baseados em busca heurística, algoritmos simbólicos, algoritmos aproximados, extensões de processos de decisão markovianos e aplicações, o problema de aprendizado por reforço (RL), o problema de predição e solução por diferença temporal, programação dinâmica estocástica, algoritmos Q-Learning, Sarsa e Prioritized Sweeping, aproximação espacial, aproximação temporal, aprendizado em Batch e transferência de aprendizado. 1. Introdução e motivação. A tarefa de planejamento. Planejamento clássico e probabilístico. 2. Conceitos básicos sobre processos de decisão markovianos e a existência de soluções. Processos de decisão markovianos fatorados. 3. Algoritmos básicos para MDPs: iteração de política, iteração de valor e formulação usando programação linear. 4. Algoritmos baseados em busca heurística para MDPs: LAO*, RTDP e extensões, entre outros. 5. Algoritmos simbólicos para MDPs fatorados: SPUDD, LAO* simbólico entre outros. 6. Algoritmos aproximados para MDPs, UCT, uso de heurísticas inadmissíveis, iteração de política aproximada e programação linear aproximada. 7. Extensões de processos de decisão markovianos: MDPs contínuos, MDPs com probabilidades imprecisas, MDPs relacionais. 8. Aplicações de Planejamento probabilístico: robótica, planejamento de processos industriais, logística/transporte, composição de serviços web, etc. 9. Predição em RL: Monte Carlo e Diferença Temporal. 10. Controle em RL: Programação Dinâmica Estocástica. 11. Algoritmos Tradicionais de RL: Q-Learning, Sarsa, Sweeping. 12. Aproximação Espacial em RL: CMAC. 13. Aproximação Temporal em RL: options. 14. Aprendizado por Reforço em Batch. 15. Transferência de Aprendizado.<br/>
**Bibliografia**: [1] Mausam and Andrey Kolobov. Planning with Markov Decision Processes: An AI Perspective. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool publishers, 2012. [2] Csaba Szepesvari. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool publishers, 2013. [3] Sutton and Barto. Reinforcement Learning: an Introduction, 1998. [4] Van Otterlo and Wiering. Reinforcement Learning, 2012. [5] Bertsekas. Abstract Dynamic Programming, 2013. [6] C. Guestrin, D. Koller, R. Parr and S. Venkataraman. Efficient solution algorithms for factored MDPs. JAIR 19, pp. 399-468, 2003. [7] C. Boutilier, T. Dean and S. Hanks. Decision-Theoretic planning: Structural assumptions and computational leverage. JAIR 11, pp. 1-94, 1999. [8] Stuart Russel and Peter Norvig. Artificial Intelligence: A Modern Approach, 2 ed., Prentice Hall, 2002. [9] Dana Nau, Malik Ghallab, and Paolo Traverso. Automated Planning: Theory & Practice. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2004.<br/>
